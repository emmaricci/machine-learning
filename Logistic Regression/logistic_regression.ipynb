{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f30a4da",
   "metadata": {},
   "source": [
    "# Logistic Regression Predictor – Heart Disease"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee068ab",
   "metadata": {},
   "source": [
    "### Project Description:\n",
    "This project explores the application of logistic regression for predicting the 10-year risk of coronary heart disease (CHD) using the publicly available Framingham Heart Study dataset. Through implementation of both custom gradient descent and scikit-learn’s logistic regression models, this project aims to compare performance across regularized and non-regularized models. The analysis includes model training, testing, evaluation, and exploration of performance challenges.\n",
    "\n",
    "### Objectives:\n",
    "* Implement logistic regression from scratch using gradient descent, including both standard and L2-regularized versions.\n",
    "* Evaluate and compare model performance using log loss, confusion matrix, and classification metrics (precision, recall, f1-score).\n",
    "* Use scikit-learn’s LogisticRegression to benchmark results and observe convergence behaviors.\n",
    "* Analyze the effect of class imbalance on model performance, particularly on recall for the minority class.\n",
    "\n",
    "### Public dataset source:\n",
    "[Kaggle Heart Disease Prediction Data Set](https://www.kaggle.com/datasets/dileep070/heart-disease-prediction-using-logistic-regression)\n",
    "The dataset is publically available on the Kaggle website, and it is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. The classification goal is to predict whether the patient has 10-year risk of future coronary heart disease (CHD). The dataset provides the patients’ information. It includes over 4,000 records and 15 attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "19a662aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import autograd\n",
    "from autograd import grad\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79dd1b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>currentSmoker</th>\n",
       "      <th>cigsPerDay</th>\n",
       "      <th>BPMeds</th>\n",
       "      <th>prevalentStroke</th>\n",
       "      <th>prevalentHyp</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>totChol</th>\n",
       "      <th>sysBP</th>\n",
       "      <th>diaBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>heartRate</th>\n",
       "      <th>glucose</th>\n",
       "      <th>TenYearCHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.97</td>\n",
       "      <td>80.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>28.73</td>\n",
       "      <td>95.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>127.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.34</td>\n",
       "      <td>75.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.58</td>\n",
       "      <td>65.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>23.10</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   male  age  education  currentSmoker  cigsPerDay  BPMeds  prevalentStroke  \\\n",
       "0     1   39        4.0              0         0.0     0.0                0   \n",
       "1     0   46        2.0              0         0.0     0.0                0   \n",
       "2     1   48        1.0              1        20.0     0.0                0   \n",
       "3     0   61        3.0              1        30.0     0.0                0   \n",
       "4     0   46        3.0              1        23.0     0.0                0   \n",
       "\n",
       "   prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  heartRate  glucose  \\\n",
       "0             0         0    195.0  106.0   70.0  26.97       80.0     77.0   \n",
       "1             0         0    250.0  121.0   81.0  28.73       95.0     76.0   \n",
       "2             0         0    245.0  127.5   80.0  25.34       75.0     70.0   \n",
       "3             1         0    225.0  150.0   95.0  28.58       65.0    103.0   \n",
       "4             0         0    285.0  130.0   84.0  23.10       85.0     85.0   \n",
       "\n",
       "   TenYearCHD  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           1  \n",
       "4           0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Establish file path and import data\n",
    "path = 'heart_disease_prediction_data.csv'\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c915727b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>currentSmoker</th>\n",
       "      <th>cigsPerDay</th>\n",
       "      <th>BPMeds</th>\n",
       "      <th>prevalentStroke</th>\n",
       "      <th>prevalentHyp</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>totChol</th>\n",
       "      <th>sysBP</th>\n",
       "      <th>diaBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>heartRate</th>\n",
       "      <th>glucose</th>\n",
       "      <th>TenYearCHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4238.000000</td>\n",
       "      <td>4238.000000</td>\n",
       "      <td>4133.000000</td>\n",
       "      <td>4238.000000</td>\n",
       "      <td>4209.000000</td>\n",
       "      <td>4185.000000</td>\n",
       "      <td>4238.000000</td>\n",
       "      <td>4238.000000</td>\n",
       "      <td>4238.000000</td>\n",
       "      <td>4188.000000</td>\n",
       "      <td>4238.000000</td>\n",
       "      <td>4238.000000</td>\n",
       "      <td>4219.000000</td>\n",
       "      <td>4237.000000</td>\n",
       "      <td>3850.000000</td>\n",
       "      <td>4238.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.429212</td>\n",
       "      <td>49.584946</td>\n",
       "      <td>1.978950</td>\n",
       "      <td>0.494101</td>\n",
       "      <td>9.003089</td>\n",
       "      <td>0.029630</td>\n",
       "      <td>0.005899</td>\n",
       "      <td>0.310524</td>\n",
       "      <td>0.025720</td>\n",
       "      <td>236.721585</td>\n",
       "      <td>132.352407</td>\n",
       "      <td>82.893464</td>\n",
       "      <td>25.802008</td>\n",
       "      <td>75.878924</td>\n",
       "      <td>81.966753</td>\n",
       "      <td>0.151958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.495022</td>\n",
       "      <td>8.572160</td>\n",
       "      <td>1.019791</td>\n",
       "      <td>0.500024</td>\n",
       "      <td>11.920094</td>\n",
       "      <td>0.169584</td>\n",
       "      <td>0.076587</td>\n",
       "      <td>0.462763</td>\n",
       "      <td>0.158316</td>\n",
       "      <td>44.590334</td>\n",
       "      <td>22.038097</td>\n",
       "      <td>11.910850</td>\n",
       "      <td>4.080111</td>\n",
       "      <td>12.026596</td>\n",
       "      <td>23.959998</td>\n",
       "      <td>0.359023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>83.500000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>15.540000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>23.070000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>25.400000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>89.875000</td>\n",
       "      <td>28.040000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>696.000000</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>142.500000</td>\n",
       "      <td>56.800000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              male          age    education  currentSmoker   cigsPerDay  \\\n",
       "count  4238.000000  4238.000000  4133.000000    4238.000000  4209.000000   \n",
       "mean      0.429212    49.584946     1.978950       0.494101     9.003089   \n",
       "std       0.495022     8.572160     1.019791       0.500024    11.920094   \n",
       "min       0.000000    32.000000     1.000000       0.000000     0.000000   \n",
       "25%       0.000000    42.000000     1.000000       0.000000     0.000000   \n",
       "50%       0.000000    49.000000     2.000000       0.000000     0.000000   \n",
       "75%       1.000000    56.000000     3.000000       1.000000    20.000000   \n",
       "max       1.000000    70.000000     4.000000       1.000000    70.000000   \n",
       "\n",
       "            BPMeds  prevalentStroke  prevalentHyp     diabetes      totChol  \\\n",
       "count  4185.000000      4238.000000   4238.000000  4238.000000  4188.000000   \n",
       "mean      0.029630         0.005899      0.310524     0.025720   236.721585   \n",
       "std       0.169584         0.076587      0.462763     0.158316    44.590334   \n",
       "min       0.000000         0.000000      0.000000     0.000000   107.000000   \n",
       "25%       0.000000         0.000000      0.000000     0.000000   206.000000   \n",
       "50%       0.000000         0.000000      0.000000     0.000000   234.000000   \n",
       "75%       0.000000         0.000000      1.000000     0.000000   263.000000   \n",
       "max       1.000000         1.000000      1.000000     1.000000   696.000000   \n",
       "\n",
       "             sysBP        diaBP          BMI    heartRate      glucose  \\\n",
       "count  4238.000000  4238.000000  4219.000000  4237.000000  3850.000000   \n",
       "mean    132.352407    82.893464    25.802008    75.878924    81.966753   \n",
       "std      22.038097    11.910850     4.080111    12.026596    23.959998   \n",
       "min      83.500000    48.000000    15.540000    44.000000    40.000000   \n",
       "25%     117.000000    75.000000    23.070000    68.000000    71.000000   \n",
       "50%     128.000000    82.000000    25.400000    75.000000    78.000000   \n",
       "75%     144.000000    89.875000    28.040000    83.000000    87.000000   \n",
       "max     295.000000   142.500000    56.800000   143.000000   394.000000   \n",
       "\n",
       "        TenYearCHD  \n",
       "count  4238.000000  \n",
       "mean      0.151958  \n",
       "std       0.359023  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       0.000000  \n",
       "max       1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "11d6dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_model(X,y_true,alpha,max_its):\n",
    "    # Initialize weights\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros(n_features)\n",
    "    b = 0.0\n",
    "    cost_history = []\n",
    "\n",
    "    for k in range(max_its):\n",
    "        # Computer linear combination\n",
    "        # Equivalent to X^T dot W\n",
    "        z = X @ w + b\n",
    "        #Clip z to prevent overflow in exp\n",
    "        # z = np.clip(z, -500, 500)\n",
    "        # Apply the sigmoid function to get prediction\n",
    "        y_pred = 1 / (1 + np.exp(-z))\n",
    "        \n",
    "        # Clip for numerical stability\n",
    "        epsilon = 1e-10\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon) \n",
    "        \n",
    "        # Compute loss\n",
    "        log_loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "        # Gradient descent\n",
    "        grad_w = (X.T @ (y_pred - y_true)) / n_samples\n",
    "        grad_b = np.sum(y_pred - y_true) / n_samples\n",
    "\n",
    "        # Update parameters\n",
    "        w -= alpha * grad_w\n",
    "        b -= alpha * grad_b\n",
    "\n",
    "        # Recompute loss\n",
    "        cost_history.append(log_loss)\n",
    "\n",
    "        # Print every 100 iterations\n",
    "        if k % 100 == 0:\n",
    "            print(f\"Iteration {k}: log loss = {log_loss:.4f}\")\n",
    "    return cost_history, w, b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f226c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign features and target\n",
    "cols = ['age','education','currentSmoker','cigsPerDay','BPMeds','prevalentStroke','prevalentHyp','diabetes','totChol','sysBP','diaBP','BMI','heartRate','glucose'] \n",
    "X = df[cols].values\n",
    "X = np.nan_to_num(X, nan=0.0, posinf=1e6, neginf=-1e6) # There are Nan values in X\n",
    "y = df['TenYearCHD'].values # 10 year risk of coronary heart disease CHD\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "db59f3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: log loss = 0.6931\n",
      "Iteration 100: log loss = 0.4344\n",
      "Iteration 200: log loss = 0.4291\n",
      "Iteration 300: log loss = 0.4254\n",
      "Iteration 400: log loss = 0.4229\n",
      "Iteration 500: log loss = 0.4210\n",
      "Iteration 600: log loss = 0.4196\n",
      "Iteration 700: log loss = 0.4185\n",
      "Iteration 800: log loss = 0.4177\n",
      "Iteration 900: log loss = 0.4170\n",
      "Learned weights: [ 0.01299961 -0.00288027 -0.00032512  0.01063011  0.00043393  0.00014152\n",
      "  0.00235764  0.00033767 -0.0039659   0.01624741 -0.01501058 -0.01797544\n",
      " -0.03121035  0.00527281]\n",
      "Intercept (b): -0.0009814646746594351\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "cost_history, w, b = logistic_model(X_train, y_train, alpha=0.0001, max_its=1000)\n",
    "\n",
    "print(\"Learned weights:\", w)\n",
    "print(\"Intercept (b):\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b827ecb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Log Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a352d7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_predict(X,y_true,w,b):\n",
    "    z = X @ w + b\n",
    "    y_pred = 1 / (1 + np.exp(-z))\n",
    "    log_loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "534e50c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.4158411852148352\n"
     ]
    }
   ],
   "source": [
    "test_loss = logit_predict(X_test, y_test, w , b)\n",
    "\n",
    "print(\"Test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25d2c93",
   "metadata": {},
   "source": [
    "The training loss over time smoothly decreases with no erratic jumps and approaches a typical value for a decent classifier. The test loss is better than random and close to the training loss, which, for a simple logistic regression model without regularization, is a good baseline.\n",
    "\n",
    "Training a model on an imbalanced dataset (classe) might cause it to favor the majority and be wrong in the minority, leading to a higher loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc684ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([2510,  456]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0dc041",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f3f08f",
   "metadata": {},
   "source": [
    "L2 regularization (aka Ridge penalty) helps to prevent overfitting by penalizing large weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "514c942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_logistic_model(X, y_true, alpha=0.01, max_its=1000, reg_lambda=0.1):\n",
    "    n_samples, n_features = X.shape\n",
    "    # w = np.zeros(n_features)\n",
    "    w = np.random.normal(0, 0.01, size=n_features)\n",
    "    b = 0.0\n",
    "    cost_history = []\n",
    "    \n",
    "    for k in range(max_its):\n",
    "        z = X @ w + b\n",
    "        y_pred = 1 / (1 + np.exp(-z))\n",
    "        y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
    "\n",
    "        # L2-regularized loss\n",
    "        log_loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        l2_penalty = (reg_lambda / (2 * n_samples)) * np.sum(w ** 2)\n",
    "        total_loss = log_loss + l2_penalty\n",
    "\n",
    "        # Gradient with L2\n",
    "        grad_w = (X.T @ (y_pred - y_true)) / n_samples + (reg_lambda / n_samples) * w\n",
    "        grad_b = np.sum(y_pred - y_true) / n_samples\n",
    "\n",
    "        w -= alpha * grad_w\n",
    "        b -= alpha * grad_b\n",
    "\n",
    "        cost_history.append(total_loss)\n",
    "\n",
    "        if k % 100 == 0:\n",
    "            print(f\"Iteration {k}: total loss = {total_loss:.4f}\")\n",
    "    return cost_history, w, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "64d73ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: total loss = 0.6813\n",
      "Iteration 100: total loss = 0.4416\n",
      "Iteration 200: total loss = 0.4350\n",
      "Iteration 300: total loss = 0.4306\n",
      "Iteration 400: total loss = 0.4272\n",
      "Iteration 500: total loss = 0.4247\n",
      "Iteration 600: total loss = 0.4227\n",
      "Iteration 700: total loss = 0.4211\n",
      "Iteration 800: total loss = 0.4198\n",
      "Iteration 900: total loss = 0.4188\n",
      "Learned weights: [ 0.00573789 -0.00780144 -0.00715728  0.00818497 -0.00283553 -0.0033222\n",
      "  0.00508599  0.00311879 -0.00370077  0.01675258 -0.01352605 -0.01552556\n",
      " -0.03011105  0.00530281]\n",
      "Intercept (b): -0.0009019435868228638\n"
     ]
    }
   ],
   "source": [
    "cost_history, w, b = L2_logistic_model(X_train, y_train, alpha=0.0001, max_its=1000)\n",
    "\n",
    "print(\"Learned weights:\", w)\n",
    "print(\"Intercept (b):\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9f97768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_logit_predict(X,y_true,w,b, reg_lambda = 0.1):\n",
    "    z = X @ w + b\n",
    "    y_pred = 1 / (1 + np.exp(-z))\n",
    "    log_loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    l2_penalty = (reg_lambda / (2 * n_samples)) * np.sum(w ** 2)\n",
    "    total_loss = log_loss + l2_penalty\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "371d3fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.4164469804049391\n"
     ]
    }
   ],
   "source": [
    "total_loss = L2_logit_predict(X_test, y_test, w , b)\n",
    "\n",
    "print(\"Test loss:\", total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9450eabf",
   "metadata": {},
   "source": [
    "L2 training loss is slightly higher because I added a penalty term to discourage large weights, but the difference between the two is very small. Test loss is almost identical, which may suggest that the model is not overfitting in the first place. Also, the data are relatively clean and low dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c8daef",
   "metadata": {},
   "source": [
    "## Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6c123059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn Test Log Loss: 0.38161662995104506\n"
     ]
    }
   ],
   "source": [
    "# Fit sklearn logistic regression\n",
    "reg_lambda = 0.1\n",
    "# model = LogisticRegression(penalty='l2', C=1/reg_lambda, solver='lbfgs', max_iter=1000)\n",
    "model = LogisticRegression(penalty='l2', C=1/reg_lambda, solver='lbfgs', max_iter=5000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1] # Raw probabilities\n",
    "y_pred = model.predict(X_test) # Predicted classes\n",
    "\n",
    "# Evaluate\n",
    "test_log_loss = log_loss(y_test, y_pred_prob)\n",
    "\n",
    "print(\"Scikit-learn Test Log Loss:\", test_log_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "26a6836b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8584905660377359"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean accuracy\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ab210c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1076    8]\n",
      " [ 172   16]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "732f13cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.99      0.92      1084\n",
      "           1       0.67      0.09      0.15       188\n",
      "\n",
      "    accuracy                           0.86      1272\n",
      "   macro avg       0.76      0.54      0.54      1272\n",
      "weighted avg       0.83      0.86      0.81      1272\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc5348",
   "metadata": {},
   "source": [
    "The recall may be very low once again because of the class imbalance. Could potentially use class weighting to improve this performance by penalizing when the model misclassifies the minority class more heavily."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
